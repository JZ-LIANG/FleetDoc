Please save your target first
2020-08-14 17:31:19,298-WARNING: set enable_sequential_execution=True since you have enable the recompute strategy
W0814 17:31:19.566316 64337 device_context.cc:268] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.2
W0814 17:31:19.572417 64337 device_context.cc:276] device: 0, cuDNN Version: 7.4.
I0814 17:31:37.467701 64337 build_strategy.cc:361] set enable_sequential_execution:1
W0814 17:31:37.836570 64337 fuse_all_reduce_op_pass.cc:75] Find all_reduce operators: 398. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 251.
W0814 17:32:11.538257 70314 operator.cc:189] matmul_grad raises an exception paddle::memory::allocation::BadAlloc, 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const
1   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
2   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, void> >::_M_invoke(std::_Any_data const&)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOp(paddle::framework::details::OpHandleBase*, std::shared_ptr<paddle::framework::BlockingQueue<unsigned long> > const&, unsigned long*)
4   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOpSync(paddle::framework::details::OpHandleBase*)
5   paddle::framework::details::ComputationOpHandle::RunImpl()
6   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
7   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
8   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
9   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
10  paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>::CalcInputGrad(paddle::framework::ExecutionContext const&, paddle::framework::Tensor const&, bool, bool, paddle::framework::Tensor const&, bool, bool, paddle::framework::Tensor*) const
12  paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>::MatMul(paddle::framework::ExecutionContext const&, paddle::framework::Tensor const&, bool, paddle::framework::Tensor const&, bool, paddle::framework::Tensor*) const
13  paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
14  paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
15  paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
16  paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
17  paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
18  paddle::memory::allocation::Allocator::Allocate(unsigned long)
19  paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
20  paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
21  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
22  paddle::memory::allocation::BadAlloc::BadAlloc(std::string, char const*, int)
23  paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 844.691162MB memory on GPU 0, available memory is only 811.875000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/home/jingqinghe/Paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)
Traceback (most recent call last):
  File "bert_recompute.py", line 38, in <module>
    fetch_list=[model.loss.name])
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 1087, in run
    six.reraise(*sys.exc_info())
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 1085, in run
    return_merged=return_merged)
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 1179, in _run_impl
    return_merged=return_merged)
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 898, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
RuntimeError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const
1   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
2   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, void> >::_M_invoke(std::_Any_data const&)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOp(paddle::framework::details::OpHandleBase*, std::shared_ptr<paddle::framework::BlockingQueue<unsigned long> > const&, unsigned long*)
4   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOpSync(paddle::framework::details::OpHandleBase*)
5   paddle::framework::details::ComputationOpHandle::RunImpl()
6   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
7   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
8   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
9   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
10  paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>::CalcInputGrad(paddle::framework::ExecutionContext const&, paddle::framework::Tensor const&, bool, bool, paddle::framework::Tensor const&, bool, bool, paddle::framework::Tensor*) const
12  paddle::operators::MatMulGradKernel<paddle::platform::CUDADeviceContext, float>::MatMul(paddle::framework::ExecutionContext const&, paddle::framework::Tensor const&, bool, paddle::framework::Tensor const&, bool, paddle::framework::Tensor*) const
13  paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
14  paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
15  paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
16  paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
17  paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
18  paddle::memory::allocation::Allocator::Allocate(unsigned long)
19  paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
20  paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
21  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
22  paddle::memory::allocation::BadAlloc::BadAlloc(std::string, char const*, int)
23  paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 844.691162MB memory on GPU 0, available memory is only 811.875000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/home/jingqinghe/Paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)

Please save your target first
2020-08-14 17:35:25,077-WARNING: set enable_sequential_execution=True since you have enable the recompute strategy
W0814 17:35:25.352218 108280 device_context.cc:268] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.2
W0814 17:35:25.358108 108280 device_context.cc:276] device: 0, cuDNN Version: 7.4.
I0814 17:35:42.576722 108280 build_strategy.cc:361] set enable_sequential_execution:1
W0814 17:35:42.930178 108280 fuse_all_reduce_op_pass.cc:75] Find all_reduce operators: 398. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 251.
W0814 17:36:20.899752 114964 operator.cc:189] mul_grad raises an exception paddle::memory::allocation::BadAlloc, 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const
1   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
2   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, void> >::_M_invoke(std::_Any_data const&)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOp(paddle::framework::details::OpHandleBase*, std::shared_ptr<paddle::framework::BlockingQueue<unsigned long> > const&, unsigned long*)
4   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOpSync(paddle::framework::details::OpHandleBase*)
5   paddle::framework::details::ComputationOpHandle::RunImpl()
6   paddle::framework::details::OpHandleBase::RunAndRecordEvent(std::function<void ()> const&)
7   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
8   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
9   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
10  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
11  paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
12  paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
13  paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
14  paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
15  paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
16  paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
17  paddle::memory::allocation::Allocator::Allocate(unsigned long)
18  paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
19  paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
20  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
21  paddle::memory::allocation::BadAlloc::BadAlloc(std::string, char const*, int)
22  paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 407.062744MB memory on GPU 0, available memory is only 353.875000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/home/jingqinghe/Paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)
Traceback (most recent call last):
  File "bert_recompute.py", line 38, in <module>
    fetch_list=[model.loss.name])
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 1087, in run
    six.reraise(*sys.exc_info())
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 1085, in run
    return_merged=return_merged)
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 1179, in _run_impl
    return_merged=return_merged)
  File "/home/jingqinghe/python-gcc482-paddle/lib/python2.7/site-packages/paddle/fluid/executor.py", line 898, in _run_parallel
    tensors = exe.run(fetch_var_names, return_merged)._move_to_list()
RuntimeError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const
1   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
2   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, void> >::_M_invoke(std::_Any_data const&)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOp(paddle::framework::details::OpHandleBase*, std::shared_ptr<paddle::framework::BlockingQueue<unsigned long> > const&, unsigned long*)
4   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOpSync(paddle::framework::details::OpHandleBase*)
5   paddle::framework::details::ComputationOpHandle::RunImpl()
6   paddle::framework::details::OpHandleBase::RunAndRecordEvent(std::function<void ()> const&)
7   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
8   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
9   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
10  std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
11  paddle::operators::MulGradKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
12  paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
13  paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
14  paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
15  paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
16  paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
17  paddle::memory::allocation::Allocator::Allocate(unsigned long)
18  paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
19  paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
20  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
21  paddle::memory::allocation::BadAlloc::BadAlloc(std::string, char const*, int)
22  paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 407.062744MB memory on GPU 0, available memory is only 353.875000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/home/jingqinghe/Paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)

