Please save your target first
2020-08-14 17:31:22,934-WARNING: set enable_sequential_execution=True since you have enable the recompute strategy
W0814 17:31:23.211829 64341 device_context.cc:268] Please NOTE: device: 2, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.2
W0814 17:31:23.216958 64341 device_context.cc:276] device: 2, cuDNN Version: 7.4.
E0814 17:31:28.859118973   69392 tcp_server_posix.cc:64]     check for SO_REUSEPORT: {"created":"@1597397488.859097896","description":"SO_REUSEPORT unavailable on compiling system","file":"src/core/lib/iomgr/socket_utils_common_posix.cc","file_line":163}
I0814 17:31:28.859953 69392 grpc_server.cc:480] Server listening on 127.0.0.1:53868 successful, selected port: 53868
I0814 17:31:37.468299 64341 build_strategy.cc:361] set enable_sequential_execution:1
W0814 17:31:37.799280 64341 fuse_all_reduce_op_pass.cc:75] Find all_reduce operators: 398. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 251.
W0814 17:32:11.853536 70208 operator.cc:189] elementwise_add raises an exception paddle::memory::allocation::BadAlloc, 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   ThreadPool::ThreadPool(unsigned long)::{lambda()#1}::operator()() const
1   std::__future_base::_State_base::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>&, bool&)
2   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, void> >::_M_invoke(std::_Any_data const&)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOp(paddle::framework::details::OpHandleBase*, std::shared_ptr<paddle::framework::BlockingQueue<unsigned long> > const&, unsigned long*)
4   paddle::framework::details::FastThreadedSSAGraphExecutor::RunOpSync(paddle::framework::details::OpHandleBase*)
5   paddle::framework::details::ComputationOpHandle::RunImpl()
6   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
7   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
8   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
9   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, float>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, double>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, int>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, long>, paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, paddle::platform::float16> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
10  paddle::operators::ElementwiseAddKernel<paddle::platform::CUDADeviceContext, float>::Compute(paddle::framework::ExecutionContext const&) const
11  paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
12  paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
13  paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
14  paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
15  paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
16  paddle::memory::allocation::Allocator::Allocate(unsigned long)
17  paddle::memory::allocation::AutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
18  paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
19  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
20  paddle::memory::allocation::BadAlloc::BadAlloc(std::string, char const*, int)
21  paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 407.437744MB memory on GPU 2, available memory is only 323.875000MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 at (/home/jingqinghe/Paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:69)


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::ParallelExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
1   paddle::framework::details::ScopeBufferedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
2   paddle::framework::details::ScopeBufferedMonitor::Apply(std::function<void ()> const&, bool)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
4   paddle::framework::SignalHandle(char const*, int)
5   paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
FatalError: A serious error (Termination signal) is detected by the operating system. at (/home/jingqinghe/Paddle/paddle/fluid/platform/init.cc:284)
  [TimeInfo: *** Aborted at 1597397534 (unix time) try "date -d @1597397534" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3890000f967) received by PID 64341 (TID 0x7fed04cad700) from PID 63847 ***]

Please save your target first
2020-08-14 17:35:28,765-WARNING: set enable_sequential_execution=True since you have enable the recompute strategy
W0814 17:35:29.043145 108284 device_context.cc:268] Please NOTE: device: 2, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.2
W0814 17:35:29.047533 108284 device_context.cc:276] device: 2, cuDNN Version: 7.4.
E0814 17:35:33.508195451  113771 tcp_server_posix.cc:64]     check for SO_REUSEPORT: {"created":"@1597397733.508178678","description":"SO_REUSEPORT unavailable on compiling system","file":"src/core/lib/iomgr/socket_utils_common_posix.cc","file_line":163}
I0814 17:35:33.509241 113771 grpc_server.cc:480] Server listening on 127.0.0.1:36503 successful, selected port: 36503
I0814 17:35:42.577270 108284 build_strategy.cc:361] set enable_sequential_execution:1
W0814 17:35:42.921612 108284 fuse_all_reduce_op_pass.cc:75] Find all_reduce operators: 398. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 251.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::ParallelExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
1   paddle::framework::details::ScopeBufferedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
2   paddle::framework::details::ScopeBufferedMonitor::Apply(std::function<void ()> const&, bool)
3   paddle::framework::details::FastThreadedSSAGraphExecutor::Run(std::vector<std::string, std::allocator<std::string> > const&, bool)
4   paddle::framework::SignalHandle(char const*, int)
5   paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
FatalError: A serious error (Termination signal) is detected by the operating system. at (/home/jingqinghe/Paddle/paddle/fluid/platform/init.cc:284)
  [TimeInfo: *** Aborted at 1597397786 (unix time) try "date -d @1597397786" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3890001a61e) received by PID 108284 (TID 0x7fd48933c700) from PID 108062 ***]

